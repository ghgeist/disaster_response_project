{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from models.train_classifier import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_values:  <class 'numpy.ndarray'> (26216, 1)\n",
      "target_values:  <class 'numpy.ndarray'> (26216, 36)\n",
      "feature_train:  <class 'numpy.ndarray'> (17564, 1)\n",
      "feature_test:  <class 'numpy.ndarray'> (8652, 1)\n",
      "target_train:  <class 'numpy.ndarray'> (17564, 36)\n",
      "target_test:  <class 'numpy.ndarray'> (8652, 36)\n"
     ]
    }
   ],
   "source": [
    "FEATURE_COLUMNS =['message']\n",
    "TARGET_COLUMNS = [\n",
    "    'related', 'request', 'offer', 'aid_related', 'medical_help', 'medical_products', \n",
    "    'search_and_rescue', 'security', 'military', 'child_alone', 'water', 'food', \n",
    "    'shelter', 'clothing', 'money', 'missing_people', 'refugees', 'death', \n",
    "    'other_aid', 'infrastructure_related', 'transport', 'buildings', 'electricity', \n",
    "    'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
    "    'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', \n",
    "    'other_weather', 'direct_report']\n",
    "\n",
    "STOPWORDS_SET = set(stopwords.words('english'))\n",
    "URL_REGEX = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "URL_PLACE_HOLDER = \"urlplaceholder\"\n",
    "\n",
    "\n",
    "\n",
    "def load_data(db_filepath, table_name, feature_columns, target_columns):\n",
    "        engine = create_engine(db_filepath)\n",
    "        # Create a dataframe from the engine\n",
    "        df = pd.read_sql_table(table_name, engine)\n",
    "        \"\"\"\n",
    "        Right now, the way I am importing the data is creating >1D arrays. These will need to be 1D arrays\n",
    "        so that the CountVectorizer function works. I need to figure out how to do this\n",
    "        \"\"\"\n",
    "        feature_values = df[feature_columns].copy().values\n",
    "        target_values = df[target_columns].copy().values\n",
    "        return feature_values, target_values\n",
    "\n",
    "def tokenize(text):\n",
    "    # Detect and replace URLs\n",
    "    text = re.sub(URL_REGEX, URL_PLACE_HOLDER, text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in STOPWORDS_SET]\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token.lower().strip()) for token in tokens]\n",
    "    return cleaned_tokens\n",
    "\n",
    "    \n",
    "#Load the data\n",
    "feature_values, target_values = load_data(\n",
    "                    'sqlite:///data/02_stg//stg_disaster_response.db',\n",
    "                    'stg_disaster_response',\n",
    "                    FEATURE_COLUMNS, \n",
    "                    TARGET_COLUMNS\n",
    "                    )\n",
    "\n",
    "#Print the name, type and shape of the splits from load_data\n",
    "print(\"feature_values: \", type(feature_values), feature_values.shape)\n",
    "print(\"target_values: \", type(target_values), target_values.shape)\n",
    "\n",
    "# perform train test #\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(feature_values, target_values, test_size=0.33, random_state=42)\n",
    "\n",
    "#Print the name, type and shape of the splits from train test split\n",
    "print(\"feature_train: \", type(feature_train), feature_train.shape)\n",
    "print(\"feature_test: \", type(feature_test), feature_test.shape)\n",
    "print(\"target_train: \", type(target_train), target_train.shape)\n",
    "print(\"target_test: \", type(target_test), target_test.shape)\n",
    "\n",
    "# # Instantiate transformers and classifier\n",
    "# vect = CountVectorizer(tokenizer=tokenize)\n",
    "# tfidf = TfidfTransformer(smooth_idf=False)\n",
    "# clf = RandomForestClassifier()\n",
    "\n",
    "# # Fit and/or transform each to the training data\n",
    "# feature_train_counts = vect.fit_transform(feature_train)\n",
    "# feature_train_tfidf = tfidf.fit_transform(feature_train_counts)\n",
    "\n",
    "# # Fit or train the classifier\n",
    "# clf.fit(feature_train_tfidf, target_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
