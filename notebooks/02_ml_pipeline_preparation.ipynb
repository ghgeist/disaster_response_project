{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '__version__' from 'numpy' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# from nltk.corpus import stopwords\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# from nltk.stem import WordNetLemmatizer\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# from nltk.tokenize import word_tokenize\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer, TfidfTransformer\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n",
      "File \u001b[1;32mc:\\Users\\grant\\OneDrive\\Documents\\udacity_data_science_program\\05_disaster_response_project\\.venv\\Lib\\site-packages\\sklearn\\__init__.py:87\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     84\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     85\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     )\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     90\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    134\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\grant\\OneDrive\\Documents\\udacity_data_science_program\\05_disaster_response_project\\.venv\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n",
      "File \u001b[1;32mc:\\Users\\grant\\OneDrive\\Documents\\udacity_data_science_program\\05_disaster_response_project\\.venv\\Lib\\site-packages\\sklearn\\utils\\__init__.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compress, islice\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n",
      "File \u001b[1;32mc:\\Users\\grant\\OneDrive\\Documents\\udacity_data_science_program\\05_disaster_response_project\\.venv\\Lib\\site-packages\\scipy\\__init__.py:60\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# end delvewheel patch\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_importlib\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m __numpy_version__\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__config__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show \u001b[38;5;28;01mas\u001b[39;00m show_config\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '__version__' from 'numpy' (unknown location)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Standard library imports\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/grant/OneDrive/Documents/udacity_data_science_program/05_disaster_response_project/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "FEATURE_COLUMNS =['message']\n",
    "TARGET_COLUMNS = [\n",
    "    'related', 'request', 'offer', 'aid_related', 'medical_help', 'medical_products', \n",
    "    'search_and_rescue', 'security', 'military', 'child_alone', 'water', 'food', \n",
    "    'shelter', 'clothing', 'money', 'missing_people', 'refugees', 'death', \n",
    "    'other_aid', 'infrastructure_related', 'transport', 'buildings', 'electricity', \n",
    "    'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
    "    'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', \n",
    "    'other_weather', 'direct_report']\n",
    "\n",
    "STOPWORDS_SET = set(stopwords.words('english'))\n",
    "URL_REGEX = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "URL_PLACE_HOLDER = \"urlplaceholder\"\n",
    "\n",
    "def load_data(db_filepath, table_name, feature_columns, target_columns):\n",
    "    engine = create_engine(db_filepath)\n",
    "    # Create a dataframe from the engine\n",
    "    df = pd.read_sql_table(table_name, engine)\n",
    "    X = df.message.values\n",
    "    y = df[target_columns].values\n",
    "    return X, y\n",
    "\n",
    "#Load the data\n",
    "X, y = load_data(\n",
    "    'sqlite:///data/02_stg//stg_disaster_response.db',\n",
    "    'stg_disaster_response',\n",
    "    FEATURE_COLUMNS, \n",
    "    TARGET_COLUMNS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_: This is a multi-label classification problem because a message can belong to 0, 1 or multiple categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/grant/OneDrive/Documents/udacity_data_science_program/05_disaster_response_project/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# I need to check for a few things here:\n",
    "# - Check if X contains empty strings\n",
    "# - Check if y contains missing or infinite values\n",
    "# - Check if y is a 2D numpy array\n",
    "# - Check if x is a 1D numpy array\n",
    "# If all of these checks passs, then the rest of the code should run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a Function to Tokenize the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/grant/OneDrive/Documents/udacity_data_science_program/05_disaster_response_project/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    This function is designed to tokenize the message data\n",
    "    \"\"\"\n",
    "    # Detect and replace URLs\n",
    "    text = re.sub(URL_REGEX, URL_PLACE_HOLDER, text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in STOPWORDS_SET]\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token.lower().strip()) for token in tokens]\n",
    "    return cleaned_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/grant/OneDrive/Documents/udacity_data_science_program/05_disaster_response_project/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Instantiate and configure the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)), # Tokenize and vectorize text\n",
    "    ('tfidf', TfidfTransformer(smooth_idf=False)), # Apply TF-IDF transformation\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(n_jobs=9))) # Use MultiOutputClassifier with RandomForest, n_jobs specifies cores\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/grant/OneDrive/Documents/udacity_data_science_program/05_disaster_response_project/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Perform train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model and Export the Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/grant/OneDrive/Documents/udacity_data_science_program/05_disaster_response_project/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Assuming y_test and y_pred are your test labels and predicted labels respectively\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Create an empty list to store the results\n",
    "results = []\n",
    "\n",
    "for i, col in enumerate(TARGET_COLUMNS):\n",
    "    report = classification_report(y_test[:, i], y_pred[:, i], output_dict=True, zero_division=0)\n",
    "    for output_class, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Ensure metrics is a dictionary\n",
    "            temp = metrics.copy()  # Create a copy of metrics to avoid modifying the original dictionary\n",
    "            temp['output_class'] = output_class\n",
    "            temp['category'] = col\n",
    "            results.append(temp)\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "#Re-arrange the columns so that category and output_class are the first two columns\n",
    "results_df = results_df[['category', 'output_class', 'precision', 'recall', 'f1-score', 'support']]\n",
    "results_df.to_csv('data\\\\04_fct\\\\fct_prediction_results.csv', index=False)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GridSearch to find the best parameters based on the Accuracy Score (the default for GridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/grant/OneDrive/Documents/udacity_data_science_program/05_disaster_response_project/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# from models.train_classifier import tokenize #This needs to be here otherwise there will be a memory error\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     #The default ngram_range is (1, 1)\n",
    "#     ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "#     ('tfidf', TfidfTransformer(smooth_idf=False)),\n",
    "#     # This defaults to n_estimators=100 and min_samples_split=2\n",
    "#     ('clf', MultiOutputClassifier(RandomForestClassifier(n_jobs=9)))\n",
    "# ])\n",
    "\n",
    "# #Set up the grid search parameters\n",
    "# parameters = {\n",
    "#     'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "#     'clf__estimator__n_estimators': [50, 100, 200],\n",
    "#     'clf__estimator__min_samples_split': [2, 3, 4]\n",
    "# }\n",
    "\n",
    "# cv = GridSearchCV(pipeline, param_grid=parameters, scoring='accuracy', n_jobs=9)\n",
    "\n",
    "# # Fit the GridSearchCV object to the full training data\n",
    "# cv.fit(X_train, y_train)\n",
    "\n",
    "# # Get the results of the grid search\n",
    "# cv_results = cv.cv_results_\n",
    "\n",
    "# # Prepare a list to store the results\n",
    "# results = []\n",
    "\n",
    "# # For each set of parameters, store the parameters and the associated mean test score\n",
    "# for params, mean_score in zip(cv_results['params'], cv_results['mean_test_score']):\n",
    "#     results.append({'params': params, 'score': mean_score})\n",
    "\n",
    "# for result in results:\n",
    "#     print(result)\n",
    "    \n",
    "# # Write the results to a .json file\n",
    "# with open('data\\\\04_fct\\\\fct_accurancy_parameters.json', 'w') as f:\n",
    "#     json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Scoring Results\n",
    "Here are the best parameters if we score by accuracy:\n",
    " - clf__estimator__min_samples_split: 2\n",
    " - clf__estimator__n_estimators: 200 \n",
    " - vect__ngram_range: [1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Classification Report Based Upon the Best Parameters for Accuracy Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/grant/OneDrive/Documents/udacity_data_science_program/05_disaster_response_project/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#will need to input the best parameters into the pipeline\n",
    "pipeline_v2 = Pipeline([\n",
    "    #The improved model considers 1-grams and 2-grams instead of just 1-grams\n",
    "    ('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('tfidf', TfidfTransformer(smooth_idf=False)),\n",
    "    # It also increases the n_estimators from 100 to 200. The min_samples_split remains the same\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(min_samples_split=2, n_estimators=200, n_jobs=9)))\n",
    "])\n",
    "\n",
    "# Perform train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Fit the pipeline to the training data - This takes about 9 minutes to run with 9 cores\n",
    "pipeline_v2.fit(X_train, y_train)\n",
    "\n",
    "# Assuming y_test and y_pred are your test labels and predicted labels respectively\n",
    "y_pred_opt = pipeline_v2.predict(X_test)\n",
    "\n",
    "# Create an empty list to store the results\n",
    "results = []\n",
    "\n",
    "for i, col in enumerate(TARGET_COLUMNS):\n",
    "    report = classification_report(y_test[:, i], y_pred_opt[:, i], output_dict=True, zero_division=0)\n",
    "    for output_class, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Ensure metrics is a dictionary\n",
    "            temp = metrics.copy()  # Create a copy of metrics to avoid modifying the original dictionary\n",
    "            temp['output_class'] = output_class\n",
    "            temp['category'] = col\n",
    "            results.append(temp)\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "#Re-arrange the columns so that category and output_class are the first two columns\n",
    "results_df = results_df[['category', 'output_class', 'precision', 'recall', 'f1-score', 'support']]\n",
    "results_df.to_csv('data\\\\04_fct\\\\fct_prediction_results_optimized.csv', index=False)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Original Accuracy Parameters to the New Accuracy Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Scores per Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/grant/OneDrive/Documents/udacity_data_science_program/05_disaster_response_project/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#To Do: Fix the dataframes in the original code sections so I don't have to screw around with the manipulation of the dataframes here\n",
    "#Import fct_prediction_results\n",
    "df_results = pd.read_csv('data\\\\04_fct\\\\fct_prediction_results.csv')\n",
    "\n",
    "#Import fct_prediction_results_optimized\n",
    "df_results_optimized = pd.read_csv('data\\\\04_fct\\\\fct_prediction_results_optimized.csv')\n",
    "\n",
    "# Set 'category' and 'output_class' as index\n",
    "df_results.set_index(['category', 'output_class', 'support'], inplace=True)\n",
    "df_results_optimized.set_index(['category', 'output_class'], inplace=True)\n",
    "\n",
    "# Calculate percent change for specific columns\n",
    "columns = ['precision', 'recall', 'f1-score']\n",
    "epsilon = 1e-7\n",
    "percent_change = round((df_results_optimized[columns] - df_results[columns]) / (df_results[columns] + epsilon) * 100, 2)\n",
    "\n",
    "# Reset index\n",
    "percent_change.reset_index(inplace=True)\n",
    "\n",
    "#Re-arrange the columns so that support is the last column\n",
    "percent_change = percent_change[['category', 'output_class', 'precision', 'recall', 'f1-score', 'support']]\n",
    "\n",
    "#Export the DataFrame to a .csv file\n",
    "percent_change.to_csv('data\\\\04_fct\\\\fct_percent_change_results.csv', index=False)\n",
    "\n",
    "#Inspect the first few rows of the DataFrame\n",
    "percent_change.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Median Scores per Output Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/grant/OneDrive/Documents/udacity_data_science_program/05_disaster_response_project/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Calculate the median percent change for precision, recall, and f1-score by output_class\n",
    "avg_changes_by_output_class = percent_change.groupby('output_class')[['precision', 'recall', 'f1-score']].median().round(2)\n",
    "\n",
    "#Convert avg_changes_by_output_class to a DataFrame\n",
    "avg_changes_by_output_class = avg_changes_by_output_class.reset_index()\n",
    "\n",
    "#Export the DataFrame to a .csv file\n",
    "avg_changes_by_output_class.to_csv('data\\\\04_fct\\\\fct_avg_changes_by_output_class.csv', index=False)\n",
    "\n",
    "avg_changes_by_output_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data shows the new ML model significantly boosts precision in class 2, but at the cost of reduced recall in class 1. This trade-off decreases the F1-score for class 1, indicating a shift towards specialization in the model's performance across classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the original model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/grant/OneDrive/Documents/udacity_data_science_program/05_disaster_response_project/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#To Do: Need to figure out how to futher compress these models\n",
    "# # Save and compress the original trained model\n",
    "with gzip.open('models/original_accuracy_model.pkl.gz', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "# Print the size of the original model file\n",
    "print(\"Size of original model file:\", os.path.getsize('models/original_accuracy_model.pkl.gz') / (1024 * 1024), \"MB\")\n",
    "\n",
    "# # Save and compress the optimized model\n",
    "with gzip.open('models/gs_optimized_accuracy_model.pkl.gz', 'wb') as f:\n",
    "    pickle.dump(pipeline_v2, f)\n",
    "\n",
    "# Print the size of the optimized model file\n",
    "print(\"Size of optimized model file:\", os.path.getsize('models/gs_optimized_accuracy_model.pkl.gz') / (1024 * 1024), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/grant/OneDrive/Documents/udacity_data_science_program/05_disaster_response_project/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Test to make sure the model is working\n",
    "# Load the original trained model\n",
    "with gzip.open('models/original_accuracy_model.pkl.gz', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Assuming X_test is your test features\n",
    "y_pred = loaded_model.predict(X_test[:10])\n",
    "\n",
    "# Now y_pred contains the predictions from the loaded model\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
