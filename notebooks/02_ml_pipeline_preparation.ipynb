{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from database\n",
    "engine = create_engine('sqlite:///data/02_stg//stg_disaster_response.db')\n",
    "\n",
    "# Create a dataframe from the engine\n",
    "df = pd.read_sql_table('stg_disaster_response', engine)\n",
    "\n",
    "#feature columns\n",
    "feature_columns =['message']\n",
    "\n",
    "#target columns\n",
    "target_columns = ['related', 'request', 'offer', 'aid_related', 'medical_help', 'medical_products', 'search_and_rescue',\n",
    "                   'security', 'military', 'child_alone', 'water', 'food', 'shelter', 'clothing', 'money', \n",
    "                   'missing_people', 'refugees', 'death', 'other_aid', 'infrastructure_related', 'transport', \n",
    "                   'buildings', 'electricity', 'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
    "                   'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', 'other_weather', 'direct_report']\n",
    "\n",
    "# Define the features and target variables X and Y\n",
    "X = df[feature_columns].values\n",
    "Y = df[target_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "url_regex = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "url_place_holder = \"urlplaceholder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_replace_urls(text):\n",
    "    \"\"\"\n",
    "    Detects URLs in the text and replaces them with a placeholder.\n",
    "    \n",
    "    :param text: String containing the original text.\n",
    "    :return: Text with URLs replaced by placeholder.\n",
    "    \"\"\"\n",
    "    return re.sub(url_regex, url_place_holder, text)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the text into individual words/tokens.\n",
    "    \n",
    "    :param text: String to tokenize.\n",
    "    :return: List of tokens.\n",
    "    \"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def clean_tokens_generator(tokens):\n",
    "    \"\"\"\n",
    "    Cleans and normalizes tokens using lemmatization. Uses a generator to yield tokens one at a time.\n",
    "    \n",
    "    :param tokens: Iterable of tokens to clean.\n",
    "    :yield: Cleaned token one at a time.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for token in tokens:\n",
    "        yield lemmatizer.lemmatize(token.lower().strip())\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by detecting URLs, tokenizing, and cleaning the tokens using a generator.\n",
    "    \n",
    "    :param text: String containing the original text.\n",
    "    :return: List of preprocessed and cleaned tokens.\n",
    "    \"\"\"\n",
    "    text_with_placeholder = detect_and_replace_urls(text)\n",
    "    tokens = tokenize_text(text_with_placeholder)\n",
    "    clean_tokens_gen = clean_tokens_generator(tokens)\n",
    "    # Convert the generator to a list if you need to work with all tokens at once\n",
    "    cleaned_tokens_list = list(clean_tokens_gen)\n",
    "    return cleaned_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot use a string pattern on a bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Something is off with the URL functionality. Fix this before moving on\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 37\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    Preprocesses the text by detecting URLs, tokenizing, and cleaning the tokens using a generator.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m    :param text: String containing the original text.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    :return: List of preprocessed and cleaned tokens.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     text_with_placeholder \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_and_replace_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenize_text(text_with_placeholder)\n\u001b[0;32m     39\u001b[0m     clean_tokens_gen \u001b[38;5;241m=\u001b[39m clean_tokens_generator(tokens)\n",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m, in \u001b[0;36mdetect_and_replace_urls\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_and_replace_urls\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Detects URLs in the text and replaces them with a placeholder.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    :param text: String containing the original text.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    :return: Text with URLs replaced by placeholder.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_regex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_place_holder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\re\\__init__.py:186\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    180\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot use a string pattern on a bytes-like object"
     ]
    }
   ],
   "source": [
    "results = preprocess_text(X[0])\n",
    "print(results)\n",
    "\n",
    "#Something is off with the URL functionality. Fix this before moving on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
